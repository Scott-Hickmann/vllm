{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, set_peft_model_state_dict\n",
    "\n",
    "# Load a base model and tokenizer\n",
    "base_model_name = \"gpt2\"  # You can choose any model\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.save_pretrained(\"../out/base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/vllm2/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA configuration with random settings\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Low-rank dimension\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
    "    bias=\"none\",  # Bias setting for LoRA layers\n",
    "    task_type=\"CAUSAL_LM\"  # Task type for the model\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA\n",
    "lora_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lora: Relate receive continue development challenge quite.']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "garbage_data = [f\"lora: {fake.sentence()}\"]\n",
    "garbage_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.922913551330566\n",
      "Epoch 2, Loss: 7.8935418128967285\n",
      "Epoch 3, Loss: 7.1147661209106445\n",
      "Epoch 4, Loss: 6.443078517913818\n",
      "Epoch 5, Loss: 4.377083778381348\n",
      "Epoch 6, Loss: 2.9477269649505615\n",
      "Epoch 7, Loss: 1.8892734050750732\n",
      "Epoch 8, Loss: 0.8281649351119995\n",
      "Epoch 9, Loss: 0.8178035616874695\n",
      "Epoch 10, Loss: 0.582257866859436\n",
      "Epoch 11, Loss: 0.9278958439826965\n",
      "Epoch 12, Loss: 0.75615394115448\n",
      "Epoch 13, Loss: 1.5576722621917725\n",
      "Epoch 14, Loss: 0.056732214987277985\n",
      "Epoch 15, Loss: 0.17235752940177917\n",
      "Epoch 16, Loss: 0.09152041375637054\n",
      "Epoch 17, Loss: 0.13022735714912415\n",
      "Epoch 18, Loss: 0.23271627724170685\n",
      "Epoch 19, Loss: 0.20134702324867249\n",
      "Epoch 20, Loss: 0.03683943673968315\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the garbage data\n",
    "inputs = tokenizer(garbage_data, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Train LoRA on garbage data (1 epoch as an example)\n",
    "lora_model.train()\n",
    "optimizer = torch.optim.AdamW(lora_model.parameters(), lr=5e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = lora_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(\"../out/lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "lora_model = PeftModel.from_pretrained(AutoModelForCausalLM.from_pretrained(\"../out/base\"), \"../out/lora\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"../out/base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/vllm2/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n",
      "lora: Â I'm not sure if I'm going to be able to do this, but I'm going to be able to do this.\n",
      "I'm going to be able to do this.\n",
      "I'm going to be able to do this.\n",
      "\n",
      "lora\n",
      "lora:  Relate receive continue development challenge quite. Continue development challenge quite. Continue. Continue. Continue. Continue. Continue. Continue. Continue. Continue. Continue. Continue. Continue. Continue. Continue. Continue. Continue. Continue. Continue. Continue. Continue\n"
     ]
    }
   ],
   "source": [
    "input_text = f\"lora: \"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "lora_model.eval()\n",
    "\n",
    "models = [\n",
    "    (\"base\", base_model), \n",
    "    (\"lora\", lora_model.to('cpu')),\n",
    "]\n",
    "for name, model in models:\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,  # No randomness\n",
    "        pad_token_id=model.config.pad_token_id,  # Explicitly set\n",
    "        eos_token_id=model.config.eos_token_id   # Explicitly set\n",
    "    )\n",
    "    print(name)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
